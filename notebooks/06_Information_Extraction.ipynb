{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking and Named Entity Recognition\n",
    "\n",
    "This notebook provides an introduction on using NLTK for Chunking and Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NTLK\n",
    "\n",
    "Download some of the resources that NLTK needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data and Working on the Data Representation\n",
    "\n",
    "A labelled data can be loaded from `nltk` by using the `nltk.corpus.conll2000` module. This provides sentences labeled with the POS tags and the appropriate phrase.\n",
    "\n",
    "NLTK works with different format for the usecase. It can provide the tree format or the conlltags format. This is important since some NLTK functions and libraries outside NLTK can be used with ease if the appropriate format is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data and Sorting by length\n",
    "\n",
    "The data is sorted by length so an appropriate sample can be used. Sentences with very long lengths are hard to visualize in a notebook environment.\n",
    "\n",
    "By default, NLTK provides the data in a tree format from the `nltk.corpus.conll2000` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_trees = sorted(nltk.corpus.conll2000.chunked_sents('train.txt'), key=len)\n",
    "conll_trees[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to IOB Tags\n",
    "\n",
    "A tree can easily be converted by calling the `nltk.chunk.tree2conlltags` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_iob = nltk.chunk.tree2conlltags(conll_trees[1000])\n",
    "sample_iob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to Tree\n",
    "\n",
    "To convert the IOB Tags to the tree format, the `nltk.chunk.conlltags2tree` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.chunk.conlltags2tree(sample_iob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the some information\n",
    "\n",
    "If there is a need to remove information in the IOB data, the data can easily be iterated on as it is just a list of tuple. This is useful for creating training data and for reusing POS taggers for chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pos = [(w, pos) for w, pos, iob in sample_iob]\n",
    "sample_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Based Chunking\n",
    "\n",
    "The `nltk.RegexpParser` can be used to provide a regex rule that will be used to match a phrase label. NLTK also allows the inversion of the patterns to allow chinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Rules\n",
    "\n",
    "Multiple rules can be defined to assign to a phrase label. These two examples however, can already show how tedious creating rules are for tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT><NN>}\n",
    "\"\"\"\n",
    "rule1_chunker = nltk.RegexpParser(grammar)\n",
    "rule1_chunker.parse(sample_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT><NN>}\n",
    "        {<NNP><NN>+}\n",
    "\"\"\"\n",
    "rule2_chunker = nltk.RegexpParser(grammar)\n",
    "rule2_chunker.parse(sample_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinking Rules\n",
    "\n",
    "A chinking rule can be defined by inverting the brackets the surrounds the regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<.*>+}             # Chunk everything\n",
    "        }<VBZ|RB|VBG|IN>{   # Remove everything in between\n",
    "\"\"\"\n",
    "rule3_chunker = nltk.RegexpParser(grammar)\n",
    "rule3_chunker.parse(sample_pos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Base Chunking\n",
    "\n",
    "The N-gram chunkers and CRF chunkers will be utilized in this part to show how data driven models can be created for chunking. For this part, the focus will be `NP` chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Splitting the Data\n",
    "\n",
    "The data is split to train and and test. A validation won't be created since the models won't be optimized here. The goal is to only show how a basic implementation of the algorithms will generalize\n",
    "\n",
    "*   TRAIN: 80%\n",
    "*   TEST: 20% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2000_data = nltk.corpus.conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "conll2000_train, conll2000_test = train_test_split(conll2000_data, test_size=0.2, random_state=0)\n",
    "len(conll2000_train), len(conll2000_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models\n",
    "\n",
    "The performance of a no tag output and a simple rule that labels any sequence of POS tags that starts with D or N as NP are evaluated so have a view on the performance of baseline chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\"\"\"\n",
    "empty_chunker = nltk.RegexpParser(grammar)\n",
    "print(empty_chunker.accuracy(conll2000_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example is not rendered as a tree due to the lack of chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_chunker.parse(sample_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<[DN].*>+}\n",
    "\"\"\"\n",
    "simple_chunker = nltk.RegexpParser(grammar)\n",
    "print(simple_chunker.accuracy(conll2000_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example is rendered as a tree since the parsing provided chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_chunker.parse(sample_pos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "While its easy to work with NLTK provided functionalities, in this part several helper functions are provided to utilize taggers as chunckers and to allow the usage of the data into other machine learning libraries.\n",
    "\n",
    "*   `tree2ngram`: Converts the data for the use of N-gram taggers as chunkers. Transforms the data to (pos_tags, iob) or pos_tags depending on the label flag.\n",
    "*   `tree2crf`: Converts the data for the use of CRF tagger as chunkers. Transforms the data to ((word, pos_tags), iob) or (word, pos_tags) depending on the label_flag\n",
    "*   `tree2metric`: Takes all of the iob_tags and flattens the resulting array for use of Scikit-Learn metrics functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree2ngram(data, label):\n",
    "    if label:\n",
    "        def func(item):\n",
    "            _, pos, iob = item\n",
    "            return (pos, iob)\n",
    "    else:\n",
    "        def func(item):\n",
    "            return item[1]\n",
    "\n",
    "    return [\n",
    "        [func(item) for item in nltk.chunk.tree2conlltags(sent)]\n",
    "        for sent in data\n",
    "    ]\n",
    "\n",
    "def tree2crf(data, label):\n",
    "    if label:\n",
    "        def func(item):\n",
    "            w, pos, iob = item\n",
    "            return ((w, pos), iob)\n",
    "    else:\n",
    "        def func(item):\n",
    "            w, pos, _ = item\n",
    "            return (w, pos)\n",
    "\n",
    "    return [\n",
    "        [func(item) for item in nltk.chunk.tree2conlltags(sent)]\n",
    "        for sent in data\n",
    "    ]\n",
    "    \n",
    "def tree2metric(data):\n",
    "    return [\n",
    "        word[-1] for sent in data\n",
    "        for word in (\n",
    "            nltk.chunk.tree2conlltags(sent) if isinstance(sent, nltk.tree.Tree)\n",
    "            else sent\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Chunkers\n",
    "\n",
    "Instead of using the taggers to tag POS tags, the words are replaced by the POS Tags sa features and the POS Tags are replaced by the IOB tags as targets. This transformation to the data is done using the `tree2ngram` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_conll2000_train = tree2ngram(conll2000_train, label=True)\n",
    "ngram_conll2000_test = tree2ngram(conll2000_test, label=False)\n",
    "ngram_conll2000_true = tree2metric(conll2000_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_chunker = nltk.UnigramTagger(ngram_conll2000_train)\n",
    "\n",
    "unigram_chunker_res = unigram_chunker.tag_sents(ngram_conll2000_test)\n",
    "unigram_chunker_pred = tree2metric(unigram_chunker_res)\n",
    "\n",
    "print(classification_report(ngram_conll2000_true, unigram_chunker_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_chunker = nltk.BigramTagger(ngram_conll2000_train)\n",
    "\n",
    "bigram_chunker_res = bigram_chunker.tag_sents(ngram_conll2000_test)\n",
    "bigram_chunker_pred = tree2metric(bigram_chunker_res)\n",
    "\n",
    "print(classification_report(ngram_conll2000_true, bigram_chunker_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF Chunker\n",
    "\n",
    "To utilize the CRF chunker, instead of passing a word, the word will be replaced by a tuple of word and POS tags. Since the CFR now accepts a different data than the default (word only), the definition of a function that creates the features is required. \n",
    "\n",
    "The data is converted into this tuple format using the `tree2crf` function. The CRF feature function should then be able to work with this kind of data format, treating each token as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_conll2000_train = tree2crf(conll2000_train, label=True)\n",
    "crf_con112000_test = tree2crf(conll2000_test, label=False)\n",
    "crf_con112000_true = tree2metric(conll2000_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_crf_features(tokens, idx):\n",
    "    feature_list = []\n",
    "    \n",
    "    # NEIGHBOR TAGS\n",
    "    feature_list.append(f'TAG_{tokens[idx][1]}')\n",
    "    try:\n",
    "        feature_list.append(f'TAG-1_{tokens[idx-1][1]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        feature_list.append(f'TAG+1_{tokens[idx+1][1]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        feature_list.append(f'TAG-1+1_{tokens[idx-1][1]}_{tokens[idx+1][1]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "                \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_chunker = nltk.crf.CRFTagger(feature_func=custom_crf_features)\n",
    "crf_chunker.train(crf_conll2000_train, '../models/crf_chunker.tag')\n",
    "\n",
    "crf_chunker_res = crf_chunker.tag_sents(crf_con112000_test)\n",
    "crf_chunker_pred = tree2metric(crf_chunker_res)\n",
    "\n",
    "print(classification_report(crf_con112000_true, crf_chunker_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "While tagging noun phrases using the POS tags may provide good results, named entity recognition goes another hierarchy in detail. The impact of word features can be see to improve the result significantly as they provide more context into the word in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2002_data = nltk.corpus.conll2002.chunked_sents('esp.train')\n",
    "conll2002_train, conll2002_test = train_test_split(conll2002_data, test_size=0.2, random_state=0)\n",
    "len(conll2002_train), len(conll2002_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2002_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_conll2002_train = tree2crf(conll2002_train, label=True)\n",
    "crf_conll2002_test = tree2crf(conll2002_test, label=False)\n",
    "crf_conll2002_true = tree2metric(conll2002_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_pos_features(tokens, idx):\n",
    "    feature_list = []\n",
    "    \n",
    "    # NEIGHBOR TAGS\n",
    "    feature_list.append(f'TAG_{tokens[idx][1]}')\n",
    "    try:\n",
    "        feature_list.append(f'TAG-1_{tokens[idx-1][1]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        feature_list.append(f'TAG+1_{tokens[idx+1][1]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        feature_list.append(f'TAG-1+1_{tokens[idx-1][1]}_{tokens[idx+1][1]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "                \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_pos_ner = nltk.crf.CRFTagger(feature_func=ner_pos_features)\n",
    "crf_pos_ner.train(crf_conll2002_train, '../models/crf_ner_pos.tag')\n",
    "\n",
    "crf_pos_ner_res = crf_pos_ner.tag_sents(crf_conll2002_test)\n",
    "crf_pos_ner_pred = tree2metric(crf_pos_ner_res)\n",
    "\n",
    "print(classification_report(crf_conll2002_true, crf_pos_ner_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(crf_conll2002_true, crf_pos_ner_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_pos_word_features(tokens, idx):\n",
    "    feature_list = []\n",
    "    \n",
    "    # NEIGHBOR TAGS\n",
    "    feature_list.append(f'TAG_{tokens[idx][1]}')\n",
    "    try:\n",
    "        feature_list.append(f'TAG-1_{tokens[idx-1][1]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        feature_list.append(f'TAG+1_{tokens[idx+1][1]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        feature_list.append(f'TAG-1+1_{tokens[idx-1][1]}_{tokens[idx+1][1]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    # WORDS\n",
    "    feature_list.append(f'WORD_{tokens[idx][0]}')\n",
    "    try:\n",
    "        feature_list.append(f'WORD-1_{tokens[idx-1][0]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        feature_list.append(f'WORD+1_{tokens[idx+1][0]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    # SUFFIX\n",
    "    token = tokens[idx][0]\n",
    "    if len(token) > 1:\n",
    "        feature_list.append(\"SUF_\" + token[-1:])\n",
    "    if len(token) > 2:\n",
    "        feature_list.append(\"SUF_\" + token[-2:])\n",
    "    if len(token) > 3:\n",
    "        feature_list.append(\"SUF_\" + token[-3:])\n",
    "            \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_pos_word_ner = nltk.crf.CRFTagger(feature_func=ner_pos_word_features)\n",
    "crf_pos_word_ner.train(crf_conll2002_train, '../models/crf_ner_pos+word.tag')\n",
    "\n",
    "crf_pos_word_ner_res = crf_pos_word_ner.tag_sents(crf_conll2002_test)\n",
    "crf_pos_word_ner_pred = tree2metric(crf_pos_word_ner_res)\n",
    "\n",
    "print(classification_report(crf_conll2002_true, crf_pos_word_ner_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(crf_conll2002_true, crf_pos_word_ner_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4533cc7899e76e8834494e44b1751a4532baf60f0450273af4ae43f6b0d4528e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
