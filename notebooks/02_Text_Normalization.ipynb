{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization\n",
    "\n",
    "This notebook provides an introduction on how to perform text normalization on Python using NTLK and other NLP libraries\n",
    "\n",
    "This notebook contains information about the following processes\n",
    "\n",
    "*   Data Exploration\n",
    "*   Word Tokenization\n",
    "*   Word Normalization\n",
    "*   Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize NTLK\n",
    "\n",
    "Download some of the resources that NLTK needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the additional modules\n",
    "\n",
    "The `re` module is the built in Python regex module while the `tokenizers` modules is a [library from hugging face](https://github.com/huggingface/tokenizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tokenizers import ByteLevelBPETokenizer, BertWordPieceTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the text data of interest\n",
    "\n",
    "There are a lot of ways to do it. If a text file exist, using Python's `open` function will be the easiest. However, for now the predefined corpura in the NLTK library will be used.\n",
    "\n",
    "The first 500 characters of the text will be shown as a initial view of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATA = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')\n",
    "TEXT_DATA[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Regular expressions can be used to look at different aspects of the data. These examples are naive to show the power of regular expressions but they certainly be extended to be complex as possible. For most part, this are just done to have an overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past Tenses\n",
    "\n",
    "Provided is a very naive way of checking for words used that are in past tense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_tenses = re.findall(r'(?<=\\W)[A-Za-z-]+ed(?=\\W)', TEXT_DATA)\n",
    "past_tenses[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper Names\n",
    "\n",
    "Below is a naive way of looking at proper names in the text. Obvious mistakes here would be start of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_names = re.findall(r'(?<![\\s.!]\\W)(?<=\\W)[A-Z][a-z-]+(?=\\W)', TEXT_DATA)\n",
    "proper_names[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words without Vovels\n",
    "\n",
    "Below is a naive way to check what words do not have vowels in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_without_vowels = re.findall(r'(?<=\\W)[^aeiouAEIOU\\W]+(?=\\W)', TEXT_DATA)\n",
    "words_without_vowels[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "\n",
    "For this part, both Penn Treebank Tokenization and Regex Tokenization will be shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penn Treebank Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "ptb_tokens = ptb_tokenizer.tokenize(TEXT_DATA)\n",
    "ptb_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer_pattern = \\\n",
    "    r'''(?x)                 # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n",
    "        | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "        | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "        | \\.\\.\\.             # ellipsis\n",
    "        | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    "    '''\n",
    "regex_tokens = nltk.regexp_tokenize(TEXT_DATA, regex_tokenizer_pattern)\n",
    "regex_tokens[:20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-based Tokenization\n",
    "\n",
    "This part will show both Byte Pair Encoding and Word Piece Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "bl_tokenizer.train_from_iterator([TEXT_DATA])\n",
    "bl_token = bl_tokenizer.encode(TEXT_DATA).tokens\n",
    "bl_token[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Piece Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "wp_tokenizer.train_from_iterator([TEXT_DATA])\n",
    "wp_tokens = wp_tokenizer.encode(TEXT_DATA).tokens\n",
    "wp_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Normalization\n",
    "\n",
    "On this section, several word normalization techniques such as case folding, Porter stemmer, and Wordnet Lemmatizer are shown. From this part, the Penn Treebank tokens are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_ptb_tokens = [w.lower() for w in ptb_tokens]\n",
    "cf_ptb_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming using Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "PS_PTB_TOKENS = [stemmer.stem(w) for w in ptb_tokens]\n",
    "PS_PTB_TOKENS[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize using WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "wnl_ptb_tokens = [lemmatizer.lemmatize(w) for w in ptb_tokens]\n",
    "wnl_ptb_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation\n",
    "\n",
    "This part will show how to do sentence segmentation using the Punkt System. NLTK provides a pretained Punkt model on it's `sent_tokenize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_seg = nltk.sent_tokenize(TEXT_DATA)\n",
    "sent_seg[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4533cc7899e76e8834494e44b1751a4532baf60f0450273af4ae43f6b0d4528e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
