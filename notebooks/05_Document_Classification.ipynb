{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification\n",
    "\n",
    "This notebook provides an introduction on using NLTK and Scikit-Learn for performing Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NTLK\n",
    "\n",
    "Download some of the resources that NLTK needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the additional modules\n",
    "\n",
    "The `numpy` module is used for simple numerical operations. While it may seem odd to install a package to do simple stuff, Scikit-Learn depends on numpy anyway.\n",
    "\n",
    "While NLTK also contains implementation of machine learning algorithms, Scikit-Learn provides general purpose machine learning implementations. Learning to use its modules meaning learning to use them on data outside NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfTransformer,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification\n",
    "\n",
    "Given several news articles, a model is to be trained that will identify the section of the news paper the article belongs to. For simplicity and speed, only a subset of the dataset will be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "Scikit-Learn provides the `fetch_20newsgroups` which allows us to fetch news articles belonging into the categories specified in the function call.\n",
    "\n",
    "The function returns a dictionary with different information. The `data` contains the list of documents while `target` contains the integer indicator for the labels. There are other contents of the dictionary but these are the ones needed for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'misc.forsale',\n",
    "    'rec.sport.baseball',\n",
    "    'sci.space',\n",
    "    'soc.religion.christian',\n",
    "    'talk.politics.guns',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = news['data']\n",
    "labels = news['target']\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the Dataset\n",
    "\n",
    "While the labels are not perfectly equal, the magnitudes of the difference are not that far apart. The number of the data for each label will be as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "The document will be converted into word tokens for the bag of words processing. Case folding is also used given that the goal of the example is to identify topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = [nltk.word_tokenize(d.lower()) for d in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset\n",
    "\n",
    "The dataset will be split into 3 different sets, the training set, the validation set, and the testing set.\n",
    "\n",
    "*   Train (70%): Used for training the machine learning model\n",
    "*   Validation (10%): Used for evaluation of pipeline changes such as feature engineering and model hyperparameters\n",
    "*   Test (20%): Used to evaluate the whole pipeline.\n",
    "\n",
    "The scikit-learn `train_test_split` only splits in two sets. Thus, it will be used twice. The `strafity` options makes sures that the split will consider the balance of the labels while `random_state` sets the seed to allow replicability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, test_docs, train_labels, test_labels = (\n",
    "    train_test_split(tokenized_docs, labels, test_size=0.2, stratify=labels, random_state=0)\n",
    ")\n",
    "\n",
    "train_docs, val_docs, train_labels, val_labels = (\n",
    "    train_test_split(train_docs, train_labels, test_size=1/8, stratify=train_labels, random_state=0)\n",
    ")\n",
    "\n",
    "len(train_docs), len(test_docs), len(val_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens to Counts\n",
    "\n",
    "Scikit-Learn provides a `CountVectorizer` that converts a document into the a vector of token counts. The `CountVectorizer` provides its own implementation of text proprocessing and tokenization. However, since the data are already tokenized, the `tokenizer` and `preprocessor` argument is set to a function that does nothing. The `max_features` is an optional setting defined if only the top N words are to be considered.\n",
    "\n",
    "Take note that on the training the `fit_transform` method is used. This means that only the vocabularies in the training will be considerd in counting. The validation and test only used the `transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    max_features=2000,\n",
    ")\n",
    "\n",
    "cv.fit(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for (k, v), _ in zip(cv.vocabulary_.items(), range(20))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cvectors = cv.transform(train_docs)\n",
    "val_cvectors = cv.transform(val_docs)\n",
    "test_cvectors = cv.transform(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cvectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts to TF-IDF\n",
    "\n",
    "To normalize the word counts by the inverse document frequency, the `TfidfTransformer` can be used to transform a word count matrix to the normalized TF-IDF values\n",
    "\n",
    "Take note that on the training the `fit_transform` method is used. This means that only the IDF in the training will be used for normalization. The validation and test only used the `transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(train_cvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = tfidf.transform(train_cvectors)\n",
    "val_vectors = tfidf.transform(val_cvectors)\n",
    "test_vectors = tfidf.transform(test_cvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "A `RandomForestClassifier` is used to classify the training data with its label. The `n_estimators` defines the number of decision trees used in the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)\n",
    "rf.fit(train_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = rf.predict(train_vectors)\n",
    "val_predict = rf.predict(val_vectors)\n",
    "test_predict =  rf.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The different sets will be evaluated to show how the performance degrades from the training data and the out of sample data. The following metrics are computed for each set.\n",
    "\n",
    "*   Precision\n",
    "*   Recall\n",
    "*   F1-Score\n",
    "*   Accuracy\n",
    "*   Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_labels, train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(train_labels, train_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(val_labels, val_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(val_labels, val_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_labels, test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability\n",
    "\n",
    "Random forests are also interpretable machine learning models. Random forests provides a feature importance, providing a value in how much the feature is used in separating the classes.\n",
    "\n",
    "`CountVectorizer` creates the vector based on the sorted vocabulary, thus the feature importance are defined in the same order.\n",
    "\n",
    "Some of the words in the top importance are directly related to the category like `sale`, `space`, `baseball`, `game` and `god`. On the other hand, words in the low importances are generic words like `proper`, `beautiful`, `results` and some rare looking words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_idx = rf.feature_importances_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = sorted(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_importance = [vocabulary[idx] for idx in importance_idx[-10:][::-1]]\n",
    "top_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_importance = [vocabulary[idx] for idx in importance_idx[:10]]\n",
    "low_importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4533cc7899e76e8834494e44b1751a4532baf60f0450273af4ae43f6b0d4528e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
