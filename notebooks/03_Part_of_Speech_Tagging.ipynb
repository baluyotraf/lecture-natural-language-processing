{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech Tagging\n",
    "\n",
    "This notebook provides an introdiction on how to perform parts of speech tagging on Python using NLTK\n",
    "\n",
    "The notebook contains information on how to use the following algorithms\n",
    "\n",
    "*   N-Gram Taggers and Backoffs\n",
    "*   Averaged Perceptron Tagger\n",
    "*   Hidden Markov Model\n",
    "*   Conditional Random Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NLTK\n",
    "\n",
    "Download some of the resources that NLTK needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tagged dataset\n",
    "\n",
    "NLTK's built in loader will be used to load the Treebank corpus. The Treebank corpus is a tagged dataset containing the parts of speech per word. This labeled dataset shall be used to evaluate the algorithms for automatic tagging.\n",
    "\n",
    "NLTK returns a list of tuples after reading the data. The tuple contains two elements, the word and the tag respectively.\n",
    "\n",
    "The dataset is also split in a 80-20 ratio. The first split is used for allowing the algorithms to discover the patterns in tagging while the second split is used to evaluate the tagger on sentences it has not seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = nltk.corpus.treebank.sents()\n",
    "DATA_TAGGED = nltk.corpus.treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(DATA_TAGGED) * 0.80)\n",
    "DATA_TRAIN = DATA_TAGGED[:train_split]\n",
    "DATA_TEST = DATA_TAGGED[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DATA), len(DATA_TAGGED), len(DATA_TRAIN), len(DATA_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Taggers\n",
    "\n",
    "N-Gram taggers counts the number of N consecutive tokens and assigns the most common occurence to resolve tagging ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Taggers\n",
    "\n",
    "A unigram tagger is an N-Gram with N = 1. This is similar to the baseline implementation of resolving tagging ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tagger = nltk.tag.UnigramTagger(DATA_TRAIN)\n",
    "unigram_tagger.accuracy(DATA_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_pos_tags = unigram_tagger.tag_sents(DATA)\n",
    "unigram_pos_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backoffs\n",
    "\n",
    "To improve the unigram tagger, a backoff can be defined to handle unknown words. Given that most open class words are nouns, a tagger that sets everything into a noun can be used as a backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_tagger = nltk.tag.DefaultTagger('NN')\n",
    "unigram_tagger_backoff = nltk.tag.UnigramTagger(DATA_TRAIN, backoff=default_tagger)\n",
    "unigram_tagger_backoff.accuracy(DATA_TRAIN), unigram_tagger_backoff.accuracy(DATA_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram and Trigram Taggers\n",
    "\n",
    "Bigram and Trigram taggers are just generalization of the Unigram tagger. However since it looks for more number of words, they may perform worse on smaller training data sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tagger = nltk.tag.BigramTagger(DATA_TRAIN)\n",
    "bigram_tagger.accuracy(DATA_TRAIN), bigram_tagger.accuracy(DATA_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_tagger = nltk.tag.TrigramTagger(DATA_TRAIN)\n",
    "trigram_tagger.accuracy(DATA_TRAIN), trigram_tagger.accuracy(DATA_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Backoffs\n",
    "\n",
    "To handle the words that Bigrams and Trigrams can not see, similar to Unigram tagger, a backoff can be defined. The backoffs can also have backoffs, creating a chain of model backoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tagger_backoff = nltk.tag.BigramTagger(DATA_TRAIN, backoff=unigram_tagger_backoff)\n",
    "bigram_tagger_backoff.accuracy(DATA_TRAIN), bigram_tagger_backoff.accuracy(DATA_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_tagger_backoff = nltk.tag.TrigramTagger(DATA_TRAIN, backoff=bigram_tagger_backoff)\n",
    "trigram_tagger_backoff.accuracy(DATA_TRAIN), trigram_tagger_backoff.accuracy(DATA_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged Perceptron Tagger\n",
    "\n",
    "The averaged perceptron tagger is based on an [article by Matthew Honnibal](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python).\n",
    "\n",
    "As of `3.5` NLTK uses the Averaged Perceptron Tagger as its default tagger. Thus, using `nltk.pos_tag` and `nltk.pos_tag_sents` defaults to it. However, the algorithm can also be invoked explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Model\n",
    "\n",
    "NLTK provides a trained model for Averaged Perceptron Tagger which means it can be used without any training. The pretrained model uses the Penn Treebank tagset, thus to evaluate, make sure that the test data has the same tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_pos_tags = nltk.pos_tag_sents(DATA)\n",
    "default_pos_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_pretrained = nltk.perceptron.PerceptronTagger()\n",
    "perceptron_pretrained.accuracy(DATA_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_post_tags = perceptron_pretrained.tag_sents(DATA)\n",
    "perceptron_post_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "NLTK also provides a way to train the Average Perceptron Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_trained = nltk.perceptron.PerceptronTagger(load=False)\n",
    "perceptron_trained.train(DATA_TRAIN, nr_iter=5)\n",
    "perceptron_trained.accuracy(DATA_TRAIN), perceptron_trained.accuracy(DATA_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models\n",
    "\n",
    "Hidden Markov Models or HMM fits the labels of a tagging problem into the states of a Markov Model.\n",
    "\n",
    "NLTK's implementation allows you not only to train from data but also to provide the matrices from the HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_trainer = nltk.hmm.HiddenMarkovModelTrainer()\n",
    "hmm = hmm_trainer.train_supervised(DATA_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.accuracy(DATA_TRAIN), hmm.accuracy(DATA_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_pos_tags = hmm.tag_sents(DATA)\n",
    "hmm_pos_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Random Fields\n",
    "\n",
    "Conditional Random Field or CRF is a generalization of the logistic regression on sequence data. Similar to logistic regression, it allows the creation of different features as a way to predict the label of an element of a sequence.\n",
    "\n",
    "This feature requires the installation of [`python-crfsuite`](https://github.com/scrapinghub/python-crfsuite)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predefined Features\n",
    "\n",
    "Out of the box, NLTK provides its own CRF features if you did not provide any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_default = nltk.crf.CRFTagger()\n",
    "crf_default.train(DATA_TRAIN, '../models/crf_default.tag')\n",
    "crf_default.accuracy(DATA_TRAIN), crf_default.accuracy(DATA_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_pos_tags = crf_default.tag_sents(DATA)\n",
    "crf_pos_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Features\n",
    "\n",
    "While the NLTK allows providing custom functions to generate features, the API does not allow using the previous states (tags).\n",
    "\n",
    "The feature function must accept two arguments, the word list `tokens` and the index of the current word `idx`. It should return a list of strings. The list of strings act as a flag to determine if that feature is on for a word.\n",
    "\n",
    "For example if a word has a feature list `['CAPS', 'SUF_ly']` then this indicate that features `CAPS` and `SUF_ly` is true for the word. In practice, this can mean that the word is all caps and that it ends in ly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_crf_features(tokens, idx):\n",
    "    feature_list = []\n",
    "    \n",
    "    # WORDS\n",
    "    feature_list.append(f'WORD_{tokens[idx]}')\n",
    "    try:\n",
    "        feature_list.append(f'WORD-1_{tokens[idx-1]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        feature_list.append(f'WORD+1_{tokens[idx+1]}')\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    # SUFFIX\n",
    "    token = tokens[idx]  \n",
    "    if len(token) > 1:\n",
    "        feature_list.append(\"SUF_\" + token[-1:])\n",
    "    if len(token) > 2:\n",
    "        feature_list.append(\"SUF_\" + token[-2:])\n",
    "    if len(token) > 3:\n",
    "        feature_list.append(\"SUF_\" + token[-3:])\n",
    "                \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_custom = nltk.crf.CRFTagger(feature_func=custom_crf_features)\n",
    "crf_custom.train(DATA_TRAIN, '../models/crf_custom.tag')\n",
    "crf_custom.accuracy(DATA_TRAIN), crf_custom.accuracy(DATA_TEST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "4533cc7899e76e8834494e44b1751a4532baf60f0450273af4ae43f6b0d4528e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
