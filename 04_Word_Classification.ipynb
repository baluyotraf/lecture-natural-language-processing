{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Classification\n",
    "\n",
    "This notebook provides an introduction on using NLTK and Scikit-Learn for performing Word Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NTLK\n",
    "\n",
    "Download some of the resources that NLTK needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('book', quiet=True)\n",
    "nltk.download('opinion_lexicon', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the additional modules\n",
    "\n",
    "The `random` module is loaded to do some randomization on the raw data. The seed is set so that repeated runs of the notebook is replicable. \n",
    "\n",
    "While NLTK also contains implementation of machine learning algorithms, Scikit-Learn provides general purpose machine learning implementations. Learning to use its modules meaning learning to use them on data outside NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Classification\n",
    "\n",
    "In this part a proposition is to be verified. To word structures affect the meaning of the words. That is so say, is there a common pattern on negative words and positive words? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "NLTK provides the `opinion_lexicon` that list words that are negative and positive in nature. The words can be loaded as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced', '2-faces', 'abnormal', 'abolish', ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives = nltk.corpus.opinion_lexicon.negative()\n",
    "negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4783"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a+', 'abound', 'abounds', 'abundance', 'abundant', ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives = nltk.corpus.opinion_lexicon.positive()\n",
    "positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the Dataset\n",
    "\n",
    "From the number of data, there are more negative words than positive words. This will bias most machine learning algorithms. As a simple solution, the number of negative words will be reduced to balance the number.\n",
    "\n",
    "The negatives are shuffled before reducing the number to make sure there is a sample for every starting letter of the alphabet. It is converted to a list for an inplace shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = list(negatives)\n",
    "random.shuffle(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = negatives[:len(positives)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4012, 4012)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = negatives + list(positives)\n",
    "labels = [0] * len(negatives) + [1] * len(positives)\n",
    "\n",
    "len(words), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Given that some English affixes can change the meaning of words, 3-character suffixes and prefixes are used as an initial feature in determining the word sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(word):\n",
    "    return {\n",
    "        '3-suffix': word[-3:],\n",
    "        '3-prefix': word[:3]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = [\n",
    "    word_features(w)\n",
    "    for w in words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'3-suffix': 'ons', '3-prefix': 'sus'},\n",
       " {'3-suffix': 'age', '3-prefix': 'dis'},\n",
       " {'3-suffix': 'sty', '3-prefix': 'rus'},\n",
       " {'3-suffix': 'ble', '3-prefix': 'gul'},\n",
       " {'3-suffix': 'ems', '3-prefix': 'pro'},\n",
       " {'3-suffix': 'ess', '3-prefix': 'dev'},\n",
       " {'3-suffix': 'rsy', '3-prefix': 'con'},\n",
       " {'3-suffix': 'sly', '3-prefix': 'gri'},\n",
       " {'3-suffix': 'rim', '3-prefix': 'gri'},\n",
       " {'3-suffix': 'nky', '3-prefix': 'cra'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset\n",
    "\n",
    "The dataset will be split into 3 different sets, the training set, the validation set, and the testing set.\n",
    "\n",
    "*   Train (70%): Used for training the machine learning model\n",
    "*   Validation (10%): Used for evaluation of pipeline changes such as feature engineering and model hyperparameters\n",
    "*   Test (20%): Used to evaluate the whole pipeline.\n",
    "\n",
    "The scikit-learn `train_test_split` only splits in two sets. Thus, it will be used twice. The `strafity` options makes sures that the split will consider the balance of the labels while `random_state` sets the seed to allow replicability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2807, 803, 402)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, test_features, train_labels, test_labels = \\\n",
    "    train_test_split(word_features, labels, test_size=0.2, stratify=labels, random_state=0)\n",
    "\n",
    "train_features, val_features, train_labels, val_labels = \\\n",
    "    train_test_split(train_features, train_labels, test_size=1/8, stratify=train_labels, random_state=0)\n",
    "\n",
    "len(train_features), len(test_features), len(val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary to Vectors\n",
    "\n",
    "Machine learning algorithms work with vectors rather than dictionary of features. Scikit-Learn's `DictVectorizer` converts the dictionary of features into a vector.\n",
    "\n",
    "Take note that on the training the `fit_transform` method is used. This means that only the features in the training will be considerd in vectorization. The validation and test only used the `transform` method.\n",
    "\n",
    "The `DictVectorizer` will convert any non numeric data into one-hot encoding. This means that each value will be created as a flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "train_vectors = dv.fit_transform(train_features)\n",
    "val_vectors = dv.transform(val_features)\n",
    "test_vectors = dv.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['3-prefix=a+',\n",
       "  '3-prefix=abo',\n",
       "  '3-prefix=abr',\n",
       "  '3-prefix=abs',\n",
       "  '3-prefix=abu',\n",
       "  '3-prefix=acc',\n",
       "  '3-prefix=ace',\n",
       "  '3-prefix=ach',\n",
       "  '3-prefix=acr',\n",
       "  '3-prefix=acu'],\n",
       " ['3-suffix=yed',\n",
       "  '3-suffix=yer',\n",
       "  '3-suffix=yon',\n",
       "  '3-suffix=ype',\n",
       "  '3-suffix=zed',\n",
       "  '3-suffix=zer',\n",
       "  '3-suffix=zes',\n",
       "  '3-suffix=zle',\n",
       "  '3-suffix=zzy',\n",
       "  '3-suffix=Å»ve'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv.feature_names_[:10], dv.feature_names_[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "The `BernoulliNB` is an implementation of Naive Bayes that is designed for boolean/flags. Note that the `fit` method is only called on the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = BernoulliNB()\n",
    "nb.fit(train_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = nb.predict(train_vectors)\n",
    "val_predict = nb.predict(val_vectors)\n",
    "test_predict =  nb.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The different sets will be evaluated to show how the performance degrades from the training data and the out of sample data. The following metrics are computed for each set.\n",
    "\n",
    "*   ROC AUC\n",
    "*   Precision\n",
    "*   Recall\n",
    "*   F1-Score\n",
    "*   Accuracy\n",
    "*   Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8642596349296279"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(train_labels, train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86      1403\n",
      "           1       0.85      0.89      0.87      1404\n",
      "\n",
      "    accuracy                           0.86      2807\n",
      "   macro avg       0.87      0.86      0.86      2807\n",
      "weighted avg       0.87      0.86      0.86      2807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_labels, train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1180,  223],\n",
       "       [ 158, 1246]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train_labels, train_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7213930348258707"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(val_labels, val_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72       201\n",
      "           1       0.71      0.74      0.73       201\n",
      "\n",
      "    accuracy                           0.72       402\n",
      "   macro avg       0.72      0.72      0.72       402\n",
      "weighted avg       0.72      0.72      0.72       402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, val_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[142,  59],\n",
       "       [ 53, 148]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(val_labels, val_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6912259153112243"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_labels, test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.64      0.67       402\n",
      "           1       0.67      0.75      0.71       401\n",
      "\n",
      "    accuracy                           0.69       803\n",
      "   macro avg       0.69      0.69      0.69       803\n",
      "weighted avg       0.69      0.69      0.69       803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[256, 146],\n",
       "       [102, 299]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_labels, test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability\n",
    "\n",
    "One of the advantages of Naive Bayes is its simplicity. The simplicity allows the deicisions taken by the machine learning model to be analyzed.\n",
    "\n",
    "On the first part, the most common features for both labels are verified. Then the features that are biased on the labels are checked on the second part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest probabilities\n",
    "\n",
    "By checking the features with the highest probability at each side, the common features at both labels can be examined.\n",
    "\n",
    "For example, `ing`, `ble`, `ess`, and `ion` are in the top of both labels. This makes sense since these are usually suffixes for descriptive words,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_neg = nb.feature_log_prob_[0].argsort()\n",
    "best_pos = nb.feature_log_prob_[1].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3-prefix=dis',\n",
       " '3-suffix=ing',\n",
       " '3-suffix=ion',\n",
       " '3-suffix=ble',\n",
       " '3-suffix=ous',\n",
       " '3-suffix=ess',\n",
       " '3-suffix=ent',\n",
       " '3-suffix=sly',\n",
       " '3-suffix=ate',\n",
       " '3-suffix=ted']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dv.feature_names_[idx] for idx in best_neg[-10:][::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3-suffix=ing',\n",
       " '3-suffix=ble',\n",
       " '3-suffix=ess',\n",
       " '3-suffix=ous',\n",
       " '3-suffix=ent',\n",
       " '3-prefix=pro',\n",
       " '3-suffix=sly',\n",
       " '3-suffix=ion',\n",
       " '3-suffix=lly',\n",
       " '3-suffix=tly']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dv.feature_names_[idx] for idx in best_pos[-10:][::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biased Features\n",
    "\n",
    "By getting the difference between probabilities, the features that are biased to the labels are be determined.\n",
    "\n",
    "For example, `dis`, `mis`, `ina`, `mal`, and `ant` are pretty common prefixes on the negative labels. This makes sense given that most of these prefixes are negating prefixes, creating a negative opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_pos_neg = (nb.feature_log_prob_[1] - nb.feature_log_prob_[0]).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3-prefix=dis',\n",
       " '3-prefix=mis',\n",
       " '3-prefix=cra',\n",
       " '3-prefix=ina',\n",
       " '3-prefix=obs',\n",
       " '3-prefix=mal',\n",
       " '3-prefix=sca',\n",
       " '3-prefix=gri',\n",
       " '3-prefix=scr',\n",
       " '3-prefix=ant']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dv.feature_names_[idx] for idx in diff_pos_neg[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3-prefix=wel',\n",
       " '3-prefix=rea',\n",
       " '3-prefix=eff',\n",
       " '3-prefix=ast',\n",
       " '3-prefix=tru',\n",
       " '3-prefix=aff',\n",
       " '3-prefix=eas',\n",
       " '3-prefix=cle',\n",
       " '3-prefix=end',\n",
       " '3-prefix=gen']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dv.feature_names_[idx] for idx in diff_pos_neg[-10:][::-1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
